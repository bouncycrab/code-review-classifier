{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, precision_recall_fscore_support, accuracy_score\n",
    "from scipy.special import softmax\n",
    "from sklearn.svm import SVC\n",
    "import torch\n",
    "import random\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    EarlyStoppingCallback,\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")\n",
    "from datasets import Dataset\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After finish cleaning all of the data, clean_train.xlsx and clean_test.xlsx should be saved into the folder.\n",
    "Then, we can start on testing different models to find the model that works the best with this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax Regression (Final Model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will start with **Softmax Regression**, which is a **Multinomial Logistic Regression**. This is quite straightforward as the dataset itself have three labels, which are `delete`, `insert` and `replace`. Hence, we tested Softmax Regression, using CountVectorizer as the Feature Extraction method and SelectKBest from Chi-Squared Test as the Feature Selection method. Out of the different solver, we decided to use particularly `solver=lbfgs` because it is more computationally efficient approximating the inverse Hessian matrix and memory-efficient. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset sizes:\n",
      "Train: 875\n",
      "Test: 221\n",
      "\n",
      "Class distribution in training data:\n",
      "Expected Operation by Developer\n",
      "insert     0.336000\n",
      "delete     0.332571\n",
      "replace    0.331429\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      delete       0.74      0.76      0.75        74\n",
      "      insert       0.75      0.82      0.79        74\n",
      "     replace       0.77      0.67      0.72        73\n",
      "\n",
      "    accuracy                           0.75       221\n",
      "   macro avg       0.75      0.75      0.75       221\n",
      "weighted avg       0.75      0.75      0.75       221\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class SoftmaxClassifier:\n",
    "    def __init__(self):\n",
    "        self.pipeline = Pipeline([\n",
    "            ('features', FeatureUnion([\n",
    "                ('text', Pipeline([\n",
    "                    ('vectorizer', CountVectorizer(\n",
    "                        ngram_range=(1, 2)\n",
    "                    )),\n",
    "                    ('selector', SelectKBest(chi2, k=1010))\n",
    "                ]))\n",
    "            ])),\n",
    "            ('classifier', LogisticRegression(\n",
    "                multi_class='multinomial',\n",
    "                solver='lbfgs',  # saga is better for large datasets but we are smaller dataset so use lbfgs\n",
    "                max_iter=300000,\n",
    "                C=1,  # Moderate regularization\n",
    "                class_weight='balanced',\n",
    "                random_state=42\n",
    "            ))\n",
    "        ])\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.pipeline.fit(X, y)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.pipeline.predict(X)\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        return self.pipeline.predict_proba(X)\n",
    "\n",
    "def main():\n",
    "    # Load cleaned data    \n",
    "    train_df = pd.read_excel('../Model and Dataset/clean_train.xlsx', engine= 'openpyxl')\n",
    "    test_df = pd.read_excel('../Model and Dataset/clean_test.xlsx', engine= 'openpyxl')\n",
    "\n",
    "    print(\"Dataset sizes:\")\n",
    "    print(f\"Train: {len(train_df)}\")\n",
    "    print(f\"Test: {len(test_df)}\")\n",
    "    \n",
    "    print(\"\\nClass distribution in training data:\")\n",
    "    print(train_df['Expected Operation by Developer'].value_counts(normalize=True))\n",
    "    \n",
    "    # train softmax classifier using train data\n",
    "    classifier = SoftmaxClassifier()\n",
    "    X_train = train_df['Review Comment']\n",
    "    y_train = train_df['Expected Operation by Developer']\n",
    "    classifier.fit(X_train, y_train)\n",
    "    \n",
    "    # prediction made by test data\n",
    "    X_test = test_df['Review Comment']\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    \n",
    "    # print classification report\n",
    "    if 'Expected Operation by Developer' in test_df.columns:\n",
    "        y_test = test_df['Expected Operation by Developer']\n",
    "        print(\"\\nClassification Report:\")\n",
    "        print(classification_report(y_test, y_pred))\n",
    "\n",
    "    # save the model\n",
    "    softmax_regression_model = 'final_model.bin'\n",
    "    with open(softmax_regression_model, 'wb') as f:\n",
    "        pickle.dump(classifier, f)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machine (SVM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we tested on SVM, using the same feature extraction and feature selection methods, as to compare the performance of the Classification Model. We decided to choose the parameter `kernel='rbf'` due to its capability to work for higher dimensional data which is not linearly separable. We also use `gamma=0.01`, generating a further influence of a single training example which impacts a larger region of the feature space, to avoid overfitting.\n",
    "\n",
    "Citation: https://www.geeksforgeeks.org/gamma-parameter-in-svm/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset sizes:\n",
      "Train: 875\n",
      "Test: 221\n",
      "\n",
      "Class distribution in training data:\n",
      "Expected Operation by Developer\n",
      "insert     0.336000\n",
      "delete     0.332571\n",
      "replace    0.331429\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      delete       0.69      0.69      0.69        74\n",
      "      insert       0.66      0.70      0.68        74\n",
      "     replace       0.69      0.64      0.67        73\n",
      "\n",
      "    accuracy                           0.68       221\n",
      "   macro avg       0.68      0.68      0.68       221\n",
      "weighted avg       0.68      0.68      0.68       221\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class SVMClassifier:\n",
    "    def __init__(self):\n",
    "        self.pipeline = Pipeline([\n",
    "            ('features', FeatureUnion([\n",
    "                ('text', Pipeline([\n",
    "                    ('vectorizer', CountVectorizer(\n",
    "                        ngram_range=(1, 2)\n",
    "                    )),\n",
    "                    ('selector', SelectKBest(chi2, k=1010))\n",
    "                ]))\n",
    "            ])),\n",
    "            ('classifier', SVC(kernel=\"rbf\", # rbf can work for higher dimensional data and data that are not linearly separable\n",
    "                               C=20, # regularization parameter\n",
    "                               gamma=0.01\n",
    "                               )) \n",
    "        ])\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.pipeline.fit(X, y)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.pipeline.predict(X)\n",
    "    \n",
    "\n",
    "def main():\n",
    "    # Load cleaned data  \n",
    "    train_df = pd.read_excel('../Model and Dataset/clean_train.xlsx', engine= 'openpyxl')\n",
    "    test_df = pd.read_excel('../Model and Dataset/clean_test.xlsx', engine= 'openpyxl')\n",
    "\n",
    "    print(\"Dataset sizes:\")\n",
    "    print(f\"Train: {len(train_df)}\")\n",
    "    print(f\"Test: {len(test_df)}\")\n",
    "    \n",
    "    print(\"\\nClass distribution in training data:\")\n",
    "    print(train_df['Expected Operation by Developer'].value_counts(normalize=True))\n",
    "    \n",
    "    # train SVM classifier\n",
    "    classifier = SVMClassifier()\n",
    "    X_train = train_df['Review Comment']\n",
    "    y_train = train_df['Expected Operation by Developer']\n",
    "    classifier.fit(X_train, y_train)\n",
    "    \n",
    "    # predict using X_test\n",
    "    X_test = test_df['Review Comment']\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    \n",
    "    # print classification report\n",
    "    if 'Expected Operation by Developer' in test_df.columns:\n",
    "        y_test = test_df['Expected Operation by Developer']\n",
    "        print(\"\\nClassification Report:\")\n",
    "        print(classification_report(y_test, y_pred))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function required when codeBERT is involved"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set seed function is created for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    # If using CUDA\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    # Ensure deterministic behavior\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-Tuning CodeBERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we set seed to 42 so that everytime we run this code the result is reproducible.\n",
    "\n",
    "In the `train` function, we set parameters like `num_train_epochs=10` to provide the model more time and opportunity to optimise, however the downside is that it will take much longer training time. To address this, we implemented `EarlyStoppingCallback(early_stopping_patience=2)` to stop training the model when the selected evaluation metrics fails to improve any further on **two consecutive epoches**, which is `metric_for_best_model='accuracy'` to maintain the consistency in measuring the performance metrics throughout the project. We also set a common `learning_rate=2e-5` and `weight_decay=0.01` which will act as a penalty weightage to prevent overfitting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset sizes:\n",
      "Train: 875\n",
      "Test: 221\n",
      "\n",
      "Class distribution in training data:\n",
      "Expected Operation by Developer\n",
      "insert     0.336000\n",
      "delete     0.332571\n",
      "replace    0.331429\n",
      "Name: proportion, dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at microsoft/codebert-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc33d152d22d4766b7dc8c224e9d40f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/787 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19c01f4c45514f218fcbc89b4bc83c9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/88 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c604df94b6d4e7c8fbb51aa8dc0d721",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/221 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85982760ede448c4987d844fe5721aa4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/990 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1059, 'grad_norm': 6.518070220947266, 'learning_rate': 1.97979797979798e-05, 'epoch': 0.1}\n",
      "{'loss': 1.1356, 'grad_norm': 4.033086776733398, 'learning_rate': 1.9595959595959596e-05, 'epoch': 0.2}\n",
      "{'loss': 1.1094, 'grad_norm': 3.565566062927246, 'learning_rate': 1.9393939393939395e-05, 'epoch': 0.3}\n",
      "{'loss': 1.0827, 'grad_norm': 4.797770023345947, 'learning_rate': 1.9191919191919194e-05, 'epoch': 0.4}\n",
      "{'loss': 1.1091, 'grad_norm': 6.9524760246276855, 'learning_rate': 1.8989898989898993e-05, 'epoch': 0.51}\n",
      "{'loss': 1.08, 'grad_norm': 7.297496318817139, 'learning_rate': 1.8787878787878792e-05, 'epoch': 0.61}\n",
      "{'loss': 1.0722, 'grad_norm': 5.285859107971191, 'learning_rate': 1.8585858585858588e-05, 'epoch': 0.71}\n",
      "{'loss': 1.0535, 'grad_norm': 4.370762348175049, 'learning_rate': 1.8383838383838387e-05, 'epoch': 0.81}\n",
      "{'loss': 1.0014, 'grad_norm': 7.5923848152160645, 'learning_rate': 1.8181818181818182e-05, 'epoch': 0.91}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "795937bcafdf4795af6a9c3c4e66f5b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.9953741431236267, 'eval_accuracy': 0.48863636363636365, 'eval_f1': 0.44975738294932355, 'eval_precision': 0.4583742833742834, 'eval_recall': 0.49272030651341, 'eval_runtime': 1.6143, 'eval_samples_per_second': 54.511, 'eval_steps_per_second': 6.814, 'epoch': 1.0}\n",
      "{'loss': 1.0341, 'grad_norm': 8.023241996765137, 'learning_rate': 1.797979797979798e-05, 'epoch': 1.01}\n",
      "{'loss': 1.0751, 'grad_norm': 8.4229736328125, 'learning_rate': 1.7777777777777777e-05, 'epoch': 1.11}\n",
      "{'loss': 0.9435, 'grad_norm': 5.373987197875977, 'learning_rate': 1.7575757575757576e-05, 'epoch': 1.21}\n",
      "{'loss': 1.0002, 'grad_norm': 6.1695451736450195, 'learning_rate': 1.7373737373737375e-05, 'epoch': 1.31}\n",
      "{'loss': 0.9943, 'grad_norm': 7.081664562225342, 'learning_rate': 1.7171717171717173e-05, 'epoch': 1.41}\n",
      "{'loss': 1.0396, 'grad_norm': 9.021428108215332, 'learning_rate': 1.6969696969696972e-05, 'epoch': 1.52}\n",
      "{'loss': 0.9268, 'grad_norm': 16.00798797607422, 'learning_rate': 1.6767676767676768e-05, 'epoch': 1.62}\n",
      "{'loss': 0.9564, 'grad_norm': 8.285510063171387, 'learning_rate': 1.6565656565656567e-05, 'epoch': 1.72}\n",
      "{'loss': 0.8455, 'grad_norm': 12.347199440002441, 'learning_rate': 1.6363636363636366e-05, 'epoch': 1.82}\n",
      "{'loss': 0.9735, 'grad_norm': 13.888261795043945, 'learning_rate': 1.616161616161616e-05, 'epoch': 1.92}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67a82affb811488a8b0e2f240f4575a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8730031847953796, 'eval_accuracy': 0.625, 'eval_f1': 0.62390288893517, 'eval_precision': 0.6234035759897829, 'eval_recall': 0.6264367816091955, 'eval_runtime': 1.686, 'eval_samples_per_second': 52.194, 'eval_steps_per_second': 6.524, 'epoch': 2.0}\n",
      "{'loss': 0.8484, 'grad_norm': 11.812809944152832, 'learning_rate': 1.595959595959596e-05, 'epoch': 2.02}\n",
      "{'loss': 0.6419, 'grad_norm': 6.024279594421387, 'learning_rate': 1.575757575757576e-05, 'epoch': 2.12}\n",
      "{'loss': 0.7775, 'grad_norm': 28.596637725830078, 'learning_rate': 1.555555555555556e-05, 'epoch': 2.22}\n",
      "{'loss': 0.7409, 'grad_norm': 9.7411470413208, 'learning_rate': 1.5353535353535354e-05, 'epoch': 2.32}\n",
      "{'loss': 0.6389, 'grad_norm': 21.510696411132812, 'learning_rate': 1.5151515151515153e-05, 'epoch': 2.42}\n",
      "{'loss': 0.6122, 'grad_norm': 14.595149993896484, 'learning_rate': 1.4949494949494952e-05, 'epoch': 2.53}\n",
      "{'loss': 0.7364, 'grad_norm': 14.511611938476562, 'learning_rate': 1.4747474747474747e-05, 'epoch': 2.63}\n",
      "{'loss': 0.8046, 'grad_norm': 18.00063133239746, 'learning_rate': 1.4545454545454546e-05, 'epoch': 2.73}\n",
      "{'loss': 0.8158, 'grad_norm': 10.170394897460938, 'learning_rate': 1.4343434343434344e-05, 'epoch': 2.83}\n",
      "{'loss': 0.7483, 'grad_norm': 20.340538024902344, 'learning_rate': 1.4141414141414143e-05, 'epoch': 2.93}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31dc9b077ea94332871797a7270185cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7922959923744202, 'eval_accuracy': 0.6590909090909091, 'eval_f1': 0.6537999037999038, 'eval_precision': 0.6591174507841174, 'eval_recall': 0.6609195402298851, 'eval_runtime': 1.7025, 'eval_samples_per_second': 51.688, 'eval_steps_per_second': 6.461, 'epoch': 3.0}\n",
      "{'loss': 0.8113, 'grad_norm': 7.293466567993164, 'learning_rate': 1.3939393939393942e-05, 'epoch': 3.03}\n",
      "{'loss': 0.465, 'grad_norm': 15.744185447692871, 'learning_rate': 1.3737373737373739e-05, 'epoch': 3.13}\n",
      "{'loss': 0.4805, 'grad_norm': 11.440260887145996, 'learning_rate': 1.3535353535353538e-05, 'epoch': 3.23}\n",
      "{'loss': 0.4074, 'grad_norm': 6.900041103363037, 'learning_rate': 1.3333333333333333e-05, 'epoch': 3.33}\n",
      "{'loss': 0.6194, 'grad_norm': 48.9625129699707, 'learning_rate': 1.3131313131313132e-05, 'epoch': 3.43}\n",
      "{'loss': 0.6708, 'grad_norm': 6.321732044219971, 'learning_rate': 1.2929292929292931e-05, 'epoch': 3.54}\n",
      "{'loss': 0.4738, 'grad_norm': 9.196503639221191, 'learning_rate': 1.2727272727272728e-05, 'epoch': 3.64}\n",
      "{'loss': 0.5688, 'grad_norm': 24.89892578125, 'learning_rate': 1.2525252525252527e-05, 'epoch': 3.74}\n",
      "{'loss': 0.5792, 'grad_norm': 32.56096649169922, 'learning_rate': 1.2323232323232323e-05, 'epoch': 3.84}\n",
      "{'loss': 0.4888, 'grad_norm': 8.859649658203125, 'learning_rate': 1.2121212121212122e-05, 'epoch': 3.94}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1930a89d19924098a237688e4f3db78b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.9157833456993103, 'eval_accuracy': 0.625, 'eval_f1': 0.622768684782033, 'eval_precision': 0.6228197090266056, 'eval_recall': 0.6264367816091955, 'eval_runtime': 1.5878, 'eval_samples_per_second': 55.421, 'eval_steps_per_second': 6.928, 'epoch': 4.0}\n",
      "{'loss': 0.4918, 'grad_norm': 10.699169158935547, 'learning_rate': 1.191919191919192e-05, 'epoch': 4.04}\n",
      "{'loss': 0.2935, 'grad_norm': 18.206954956054688, 'learning_rate': 1.1717171717171718e-05, 'epoch': 4.14}\n",
      "{'loss': 0.2681, 'grad_norm': 9.819538116455078, 'learning_rate': 1.1515151515151517e-05, 'epoch': 4.24}\n",
      "{'loss': 0.4057, 'grad_norm': 21.20111656188965, 'learning_rate': 1.1313131313131314e-05, 'epoch': 4.34}\n",
      "{'loss': 0.3868, 'grad_norm': 38.27432632446289, 'learning_rate': 1.1111111111111113e-05, 'epoch': 4.44}\n",
      "{'loss': 0.2188, 'grad_norm': 25.2943172454834, 'learning_rate': 1.0909090909090909e-05, 'epoch': 4.55}\n",
      "{'loss': 0.3105, 'grad_norm': 5.3908562660217285, 'learning_rate': 1.0707070707070708e-05, 'epoch': 4.65}\n",
      "{'loss': 0.3798, 'grad_norm': 34.968994140625, 'learning_rate': 1.0505050505050507e-05, 'epoch': 4.75}\n",
      "{'loss': 0.3769, 'grad_norm': 43.84777069091797, 'learning_rate': 1.0303030303030304e-05, 'epoch': 4.85}\n",
      "{'loss': 0.4136, 'grad_norm': 75.94404602050781, 'learning_rate': 1.0101010101010103e-05, 'epoch': 4.95}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0dff8b559b9843e2a0b7739490e2f831",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.1535435914993286, 'eval_accuracy': 0.6590909090909091, 'eval_f1': 0.65625, 'eval_precision': 0.6587437254103921, 'eval_recall': 0.660536398467433, 'eval_runtime': 1.5783, 'eval_samples_per_second': 55.755, 'eval_steps_per_second': 6.969, 'epoch': 5.0}\n",
      "{'train_runtime': 274.1349, 'train_samples_per_second': 28.708, 'train_steps_per_second': 3.611, 'train_loss': 0.7317375255353523, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/_tensor_str.py:145: UserWarning: MPS: nonzero op is supported natively starting from macOS 14.0. Falling back on CPU. This may have performance implications. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/mps/operations/Indexing.mm:361.)\n",
      "  nonzero_finite_vals = torch.masked_select(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbb1c795b19344a2929cdaf2c9e8ca03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/221 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3db87f5328f4e76ac787c7393d1a430",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/28 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report on Test Set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      delete       0.73      0.61      0.66        74\n",
      "      insert       0.73      0.65      0.69        74\n",
      "     replace       0.60      0.77      0.67        73\n",
      "\n",
      "    accuracy                           0.67       221\n",
      "   macro avg       0.69      0.67      0.67       221\n",
      "weighted avg       0.69      0.67      0.67       221\n",
      "\n"
     ]
    }
   ],
   "source": [
    "set_seed(42)\n",
    "\n",
    "class CodeBERTClassifier:\n",
    "    def __init__(self, model_name='microsoft/codebert-base', max_length=256, num_labels=3):\n",
    "        self.model_name = model_name\n",
    "        self.max_length = max_length\n",
    "        self.num_labels = num_labels\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "        # Load tokenizer and model\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            self.model_name,\n",
    "            num_labels=self.num_labels\n",
    "        ).to(self.device)\n",
    "\n",
    "        # Unfreeze the entire model\n",
    "        self.unfreeze_model_layers()\n",
    "\n",
    "        self.is_trained = False\n",
    "\n",
    "    def unfreeze_model_layers(self):\n",
    "        # Ensure all model parameters are trainable\n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "    def _tokenize_function(self, examples):\n",
    "        return self.tokenizer(\n",
    "            examples['text'],\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=self.max_length\n",
    "        )\n",
    "\n",
    "    def _compute_metrics(self, eval_pred):\n",
    "        logits, labels = eval_pred\n",
    "        predictions = np.argmax(logits, axis=-1)\n",
    "\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "            labels,\n",
    "            predictions,\n",
    "            average='macro'  # Use 'macro' because classes are balanced\n",
    "        )\n",
    "        acc = accuracy_score(labels, predictions)\n",
    "        return {'accuracy': acc, 'f1': f1, 'precision': precision, 'recall': recall}\n",
    "\n",
    "    def train(self, train_dataset, val_dataset, output_dir='./results'):\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=output_dir,\n",
    "            num_train_epochs=10,             # Adjust as needed\n",
    "            per_device_train_batch_size=8,   # Reduce if memory issues occur\n",
    "            per_device_eval_batch_size=8,\n",
    "            learning_rate=2e-5,\n",
    "            weight_decay=0.01,\n",
    "            eval_strategy='epoch',\n",
    "            save_strategy='epoch',\n",
    "            load_best_model_at_end=True,\n",
    "            metric_for_best_model='accuracy',     \n",
    "            greater_is_better=True,\n",
    "            save_total_limit=1,              # Limit the number of saved models\n",
    "            logging_dir='./logs',\n",
    "            logging_steps=10,\n",
    "            seed=42\n",
    "        )\n",
    "\n",
    "        self.trainer = Trainer(\n",
    "            model=self.model,\n",
    "            args=training_args,\n",
    "            train_dataset=train_dataset,\n",
    "            eval_dataset=val_dataset,\n",
    "            tokenizer=self.tokenizer,\n",
    "            compute_metrics=self._compute_metrics,\n",
    "            callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]\n",
    "        )\n",
    "\n",
    "        self.trainer.train()\n",
    "        self.is_trained = True\n",
    "\n",
    "        return self.trainer\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Ensure the model is trained\n",
    "        if not self.is_trained:\n",
    "            raise RuntimeError(\"The model must be fitted before calling predict().\")\n",
    "\n",
    "        # Prepare data\n",
    "        df = pd.DataFrame({'text': X})\n",
    "        dataset = Dataset.from_pandas(df)\n",
    "        dataset = dataset.map(self._tokenize_function, batched=True)\n",
    "        dataset.set_format(type='torch', columns=['input_ids', 'attention_mask'])\n",
    "\n",
    "        # Make predictions\n",
    "        predictions = self.trainer.predict(dataset)\n",
    "        logits = predictions.predictions\n",
    "        y_pred = np.argmax(logits, axis=-1)\n",
    "        return y_pred  # Return numerical predictions\n",
    "\n",
    "    def evaluate(self, trainer, test_dataset):\n",
    "        # Make predictions on the test set\n",
    "        predictions = trainer.predict(test_dataset)\n",
    "        logits = predictions.predictions\n",
    "        y_pred = np.argmax(logits, axis=-1)\n",
    "        y_true = test_dataset['label']\n",
    "\n",
    "        # Return predictions and true labels\n",
    "        return y_true, y_pred\n",
    "\n",
    "def main():\n",
    "    # Load cleaned data\n",
    "    train_df = pd.read_excel('../Model and Dataset/clean_train.xlsx', engine= 'openpyxl')\n",
    "    test_df = pd.read_excel('../Model and Dataset/clean_test.xlsx', engine= 'openpyxl')\n",
    "\n",
    "    print(\"Dataset sizes:\")\n",
    "    print(f\"Train: {len(train_df)}\")\n",
    "    print(f\"Test: {len(test_df)}\")\n",
    "\n",
    "    print(\"\\nClass distribution in training data:\")\n",
    "    print(train_df['Expected Operation by Developer'].value_counts(normalize=True))\n",
    "\n",
    "    # Encode labels\n",
    "    label_encoder = LabelEncoder()\n",
    "    train_df['label'] = label_encoder.fit_transform(train_df['Expected Operation by Developer'])\n",
    "    test_df['label'] = label_encoder.transform(test_df['Expected Operation by Developer'])\n",
    "\n",
    "    # Split train_df into train and validation sets\n",
    "    train_split_df, val_df = train_test_split(\n",
    "        train_df,\n",
    "        test_size=0.1,  # 10% for validation\n",
    "        stratify=train_df['label'],  # Maintain class balance\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    # Prepare datasets for Hugging Face Trainer\n",
    "    def prepare_dataset(df):\n",
    "        return Dataset.from_pandas(\n",
    "            df[['Review Comment', 'label']].rename(columns={'Review Comment': 'text'})\n",
    "        )\n",
    "\n",
    "    train_dataset = prepare_dataset(train_split_df)\n",
    "    val_dataset = prepare_dataset(val_df)\n",
    "    test_dataset = prepare_dataset(test_df)\n",
    "\n",
    "    # Initialize classifier\n",
    "    classifier = CodeBERTClassifier(\n",
    "        model_name='microsoft/codebert-base',\n",
    "        max_length=256,\n",
    "        num_labels=len(label_encoder.classes_)\n",
    "    )\n",
    "\n",
    "    # Tokenize datasets\n",
    "    train_dataset = train_dataset.map(classifier._tokenize_function, batched=True)\n",
    "    val_dataset = val_dataset.map(classifier._tokenize_function, batched=True)\n",
    "    test_dataset = test_dataset.map(classifier._tokenize_function, batched=True)\n",
    "\n",
    "    # Set format for PyTorch\n",
    "    train_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "    val_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "    test_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "\n",
    "    # Train the model\n",
    "    trainer = classifier.train(train_dataset, val_dataset)\n",
    "\n",
    "    # Evaluate the model on the test set\n",
    "    y_pred = classifier.predict(test_df['Review Comment'])\n",
    "    y_pred_labels = label_encoder.inverse_transform(y_pred)\n",
    "    y_true_labels = test_df['Expected Operation by Developer'].values\n",
    "\n",
    "    # Print classification report\n",
    "    print(\"\\nClassification Report on Test Set:\")\n",
    "    print(classification_report(y_true_labels, y_pred_labels))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble Model (Softmax Regression + CodeBERT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we proposed the potential of an ensemble method, which is a combination of **Softmax Regression** (Classical ML Model) and **CodeBERT** (Neural Network). We are currently putting 60% weightage on the prediction result from Softmax Regression and 40% weightage on the prediction result from CodeBERT due to the fact that the Neural Network is expected to perform less ideally when the training sample is too small. However, in future, if we are able to obtain a larger training data (which will possibly improve CodeBERT performance), we can then leverage the strength of the two models accordingly by adjusting the weightage on each model. \n",
    "\n",
    "The formula for calculating the final ensemble probability for each class label:\n",
    "\n",
    "`ensemble_probs = (classical_weight * classical_probs) + (codebert_weight * codebert_probs)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at microsoft/codebert-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1dabc15798db43a1847281a42a40d34f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/787 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6c7d869ce134b01a8f7b3b86584de36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/88 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42cab4555636461c82a650ff497006d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/990 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1059, 'grad_norm': 6.518069744110107, 'learning_rate': 1.97979797979798e-05, 'epoch': 0.1}\n",
      "{'loss': 1.1356, 'grad_norm': 4.033087253570557, 'learning_rate': 1.9595959595959596e-05, 'epoch': 0.2}\n",
      "{'loss': 1.1094, 'grad_norm': 3.5655674934387207, 'learning_rate': 1.9393939393939395e-05, 'epoch': 0.3}\n",
      "{'loss': 1.0827, 'grad_norm': 4.797769069671631, 'learning_rate': 1.9191919191919194e-05, 'epoch': 0.4}\n",
      "{'loss': 1.1091, 'grad_norm': 6.952461242675781, 'learning_rate': 1.8989898989898993e-05, 'epoch': 0.51}\n",
      "{'loss': 1.08, 'grad_norm': 7.297637939453125, 'learning_rate': 1.8787878787878792e-05, 'epoch': 0.61}\n",
      "{'loss': 1.0722, 'grad_norm': 5.286148548126221, 'learning_rate': 1.8585858585858588e-05, 'epoch': 0.71}\n",
      "{'loss': 1.0535, 'grad_norm': 4.370704650878906, 'learning_rate': 1.8383838383838387e-05, 'epoch': 0.81}\n",
      "{'loss': 1.0014, 'grad_norm': 7.5914225578308105, 'learning_rate': 1.8181818181818182e-05, 'epoch': 0.91}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2636a70129341ceb6e2be6ad48db02b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.9953708052635193, 'eval_accuracy': 0.48863636363636365, 'eval_f1': 0.44975738294932355, 'eval_precision': 0.4583742833742834, 'eval_recall': 0.49272030651341, 'eval_runtime': 1.6171, 'eval_samples_per_second': 54.419, 'eval_steps_per_second': 6.802, 'epoch': 1.0}\n",
      "{'loss': 1.0341, 'grad_norm': 8.022260665893555, 'learning_rate': 1.797979797979798e-05, 'epoch': 1.01}\n",
      "{'loss': 1.0752, 'grad_norm': 8.426362037658691, 'learning_rate': 1.7777777777777777e-05, 'epoch': 1.11}\n",
      "{'loss': 0.9436, 'grad_norm': 5.368290424346924, 'learning_rate': 1.7575757575757576e-05, 'epoch': 1.21}\n",
      "{'loss': 1.0001, 'grad_norm': 6.170549392700195, 'learning_rate': 1.7373737373737375e-05, 'epoch': 1.31}\n",
      "{'loss': 0.9944, 'grad_norm': 7.071694374084473, 'learning_rate': 1.7171717171717173e-05, 'epoch': 1.41}\n",
      "{'loss': 1.0395, 'grad_norm': 9.011048316955566, 'learning_rate': 1.6969696969696972e-05, 'epoch': 1.52}\n",
      "{'loss': 0.9269, 'grad_norm': 16.006092071533203, 'learning_rate': 1.6767676767676768e-05, 'epoch': 1.62}\n",
      "{'loss': 0.9566, 'grad_norm': 8.295390129089355, 'learning_rate': 1.6565656565656567e-05, 'epoch': 1.72}\n",
      "{'loss': 0.8454, 'grad_norm': 12.328429222106934, 'learning_rate': 1.6363636363636366e-05, 'epoch': 1.82}\n",
      "{'loss': 0.973, 'grad_norm': 13.871978759765625, 'learning_rate': 1.616161616161616e-05, 'epoch': 1.92}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83a881d7c752451db3c9435ab27dfcfd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8732370734214783, 'eval_accuracy': 0.625, 'eval_f1': 0.62390288893517, 'eval_precision': 0.6234035759897829, 'eval_recall': 0.6264367816091955, 'eval_runtime': 1.6018, 'eval_samples_per_second': 54.938, 'eval_steps_per_second': 6.867, 'epoch': 2.0}\n",
      "{'loss': 0.8486, 'grad_norm': 11.732205390930176, 'learning_rate': 1.595959595959596e-05, 'epoch': 2.02}\n",
      "{'loss': 0.6422, 'grad_norm': 6.064476490020752, 'learning_rate': 1.575757575757576e-05, 'epoch': 2.12}\n",
      "{'loss': 0.7765, 'grad_norm': 28.30057144165039, 'learning_rate': 1.555555555555556e-05, 'epoch': 2.22}\n",
      "{'loss': 0.7414, 'grad_norm': 9.640498161315918, 'learning_rate': 1.5353535353535354e-05, 'epoch': 2.32}\n",
      "{'loss': 0.6388, 'grad_norm': 21.75180435180664, 'learning_rate': 1.5151515151515153e-05, 'epoch': 2.42}\n",
      "{'loss': 0.6134, 'grad_norm': 14.440499305725098, 'learning_rate': 1.4949494949494952e-05, 'epoch': 2.53}\n",
      "{'loss': 0.736, 'grad_norm': 14.513383865356445, 'learning_rate': 1.4747474747474747e-05, 'epoch': 2.63}\n",
      "{'loss': 0.8039, 'grad_norm': 17.812789916992188, 'learning_rate': 1.4545454545454546e-05, 'epoch': 2.73}\n",
      "{'loss': 0.8145, 'grad_norm': 10.13841438293457, 'learning_rate': 1.4343434343434344e-05, 'epoch': 2.83}\n",
      "{'loss': 0.7481, 'grad_norm': 20.296443939208984, 'learning_rate': 1.4141414141414143e-05, 'epoch': 2.93}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac527a0e138a480ab875310a16af4d09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7925806641578674, 'eval_accuracy': 0.6590909090909091, 'eval_f1': 0.6537999037999038, 'eval_precision': 0.6591174507841174, 'eval_recall': 0.6609195402298851, 'eval_runtime': 1.6062, 'eval_samples_per_second': 54.787, 'eval_steps_per_second': 6.848, 'epoch': 3.0}\n",
      "{'loss': 0.8123, 'grad_norm': 7.298730850219727, 'learning_rate': 1.3939393939393942e-05, 'epoch': 3.03}\n",
      "{'loss': 0.465, 'grad_norm': 15.6372652053833, 'learning_rate': 1.3737373737373739e-05, 'epoch': 3.13}\n",
      "{'loss': 0.479, 'grad_norm': 11.311602592468262, 'learning_rate': 1.3535353535353538e-05, 'epoch': 3.23}\n",
      "{'loss': 0.4074, 'grad_norm': 6.858272552490234, 'learning_rate': 1.3333333333333333e-05, 'epoch': 3.33}\n",
      "{'loss': 0.6172, 'grad_norm': 47.89690399169922, 'learning_rate': 1.3131313131313132e-05, 'epoch': 3.43}\n",
      "{'loss': 0.6718, 'grad_norm': 6.3104376792907715, 'learning_rate': 1.2929292929292931e-05, 'epoch': 3.54}\n",
      "{'loss': 0.4742, 'grad_norm': 9.289361000061035, 'learning_rate': 1.2727272727272728e-05, 'epoch': 3.64}\n",
      "{'loss': 0.5669, 'grad_norm': 25.030128479003906, 'learning_rate': 1.2525252525252527e-05, 'epoch': 3.74}\n",
      "{'loss': 0.577, 'grad_norm': 32.3376579284668, 'learning_rate': 1.2323232323232323e-05, 'epoch': 3.84}\n",
      "{'loss': 0.4899, 'grad_norm': 8.856785774230957, 'learning_rate': 1.2121212121212122e-05, 'epoch': 3.94}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f8009afb0fb4951a9f696fb0a261797",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.9170381426811218, 'eval_accuracy': 0.625, 'eval_f1': 0.622768684782033, 'eval_precision': 0.6228197090266056, 'eval_recall': 0.6264367816091955, 'eval_runtime': 1.615, 'eval_samples_per_second': 54.488, 'eval_steps_per_second': 6.811, 'epoch': 4.0}\n",
      "{'loss': 0.4931, 'grad_norm': 10.604461669921875, 'learning_rate': 1.191919191919192e-05, 'epoch': 4.04}\n",
      "{'loss': 0.2943, 'grad_norm': 19.22991943359375, 'learning_rate': 1.1717171717171718e-05, 'epoch': 4.14}\n",
      "{'loss': 0.2684, 'grad_norm': 9.718889236450195, 'learning_rate': 1.1515151515151517e-05, 'epoch': 4.24}\n",
      "{'loss': 0.4029, 'grad_norm': 21.195993423461914, 'learning_rate': 1.1313131313131314e-05, 'epoch': 4.34}\n",
      "{'loss': 0.3905, 'grad_norm': 41.43944549560547, 'learning_rate': 1.1111111111111113e-05, 'epoch': 4.44}\n",
      "{'loss': 0.2185, 'grad_norm': 25.20840835571289, 'learning_rate': 1.0909090909090909e-05, 'epoch': 4.55}\n",
      "{'loss': 0.3105, 'grad_norm': 5.518843173980713, 'learning_rate': 1.0707070707070708e-05, 'epoch': 4.65}\n",
      "{'loss': 0.3794, 'grad_norm': 35.41566467285156, 'learning_rate': 1.0505050505050507e-05, 'epoch': 4.75}\n",
      "{'loss': 0.3764, 'grad_norm': 43.88117218017578, 'learning_rate': 1.0303030303030304e-05, 'epoch': 4.85}\n",
      "{'loss': 0.4125, 'grad_norm': 75.68014526367188, 'learning_rate': 1.0101010101010103e-05, 'epoch': 4.95}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "caa92f93851e4feb936943f5a6da6468",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.1502140760421753, 'eval_accuracy': 0.6590909090909091, 'eval_f1': 0.65625, 'eval_precision': 0.6587437254103921, 'eval_recall': 0.660536398467433, 'eval_runtime': 1.6255, 'eval_samples_per_second': 54.138, 'eval_steps_per_second': 6.767, 'epoch': 5.0}\n",
      "{'train_runtime': 271.7224, 'train_samples_per_second': 28.963, 'train_steps_per_second': 3.643, 'train_loss': 0.7315815477660208, 'epoch': 5.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89b541be5c114e78a73be483033029df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/221 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04df2eff56aa4cccbb2a07c2d1735a64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/28 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report for Ensemble Model:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      delete       0.76      0.77      0.77        74\n",
      "      insert       0.77      0.77      0.77        74\n",
      "     replace       0.72      0.71      0.72        73\n",
      "\n",
      "    accuracy                           0.75       221\n",
      "   macro avg       0.75      0.75      0.75       221\n",
      "weighted avg       0.75      0.75      0.75       221\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# codeBERT part of ensemble model\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "class CodeBERTTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, model_name='microsoft/codebert-base', max_length=256, num_labels=3):\n",
    "        self.model_name = model_name\n",
    "        self.max_length = max_length\n",
    "        self.num_labels = num_labels\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.tokenizer = None\n",
    "        self.model = None\n",
    "        self.trainer = None\n",
    "        self.is_trained = False\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        # Prepare data\n",
    "        df = pd.DataFrame({'text': X, 'label': y})\n",
    "        # Perform stratified splitting using scikit-learn\n",
    "        train_df, val_df = train_test_split(\n",
    "            df,\n",
    "            test_size=0.1,\n",
    "            stratify=df['label'],\n",
    "            random_state=42\n",
    "        )\n",
    "        self.train_dataset = Dataset.from_pandas(train_df)\n",
    "        self.val_dataset = Dataset.from_pandas(val_df)\n",
    "\n",
    "        # Initialize tokenizer and model\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            self.model_name,\n",
    "            num_labels=self.num_labels\n",
    "        ).to(self.device)\n",
    "\n",
    "        # Tokenize data\n",
    "        self.train_dataset = self.train_dataset.map(self._tokenize_function, batched=True)\n",
    "        self.val_dataset = self.val_dataset.map(self._tokenize_function, batched=True)\n",
    "\n",
    "        # Set format for PyTorch\n",
    "        self.train_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "        self.val_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "\n",
    "        # Training arguments\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir='./codebert_results',\n",
    "            num_train_epochs=10,\n",
    "            per_device_train_batch_size=8,\n",
    "            per_device_eval_batch_size=8,\n",
    "            learning_rate=2e-5,\n",
    "            weight_decay=0.01,\n",
    "            eval_strategy='epoch',\n",
    "            save_strategy='epoch',\n",
    "            load_best_model_at_end=True,\n",
    "            metric_for_best_model='accuracy',\n",
    "            greater_is_better=True,\n",
    "            save_total_limit=1,\n",
    "            logging_dir='./codebert_logs',\n",
    "            logging_steps=10,\n",
    "            seed=42\n",
    "        )\n",
    "\n",
    "        # Initialize Trainer\n",
    "        self.trainer = Trainer(\n",
    "            model=self.model,\n",
    "            args=training_args,\n",
    "            train_dataset=self.train_dataset,\n",
    "            eval_dataset=self.val_dataset,\n",
    "            tokenizer=self.tokenizer,\n",
    "            compute_metrics=self._compute_metrics,\n",
    "            callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]\n",
    "        )\n",
    "\n",
    "        # Train model\n",
    "        self.trainer.train()\n",
    "        self.is_trained = True\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        # Ensure the model is trained\n",
    "        if not self.is_trained:\n",
    "            raise RuntimeError(\"The model must be fitted before calling transform().\")\n",
    "\n",
    "        # Prepare data\n",
    "        df = pd.DataFrame({'text': X})\n",
    "        dataset = Dataset.from_pandas(df)\n",
    "        dataset = dataset.map(self._tokenize_function, batched=True)\n",
    "        dataset.set_format(type='torch', columns=['input_ids', 'attention_mask'])\n",
    "\n",
    "        # Make predictions\n",
    "        predictions = self.trainer.predict(dataset)\n",
    "        logits = predictions.predictions\n",
    "        probs = softmax(logits, axis=1)\n",
    "        return probs\n",
    "\n",
    "    def _tokenize_function(self, examples):\n",
    "        return self.tokenizer(\n",
    "            examples['text'],\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=self.max_length\n",
    "        )\n",
    "\n",
    "    def _compute_metrics(self, eval_pred):\n",
    "        logits, labels = eval_pred\n",
    "        predictions = np.argmax(logits, axis=-1)\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "            labels,\n",
    "            predictions,\n",
    "            average='macro'\n",
    "        )\n",
    "        acc = accuracy_score(labels, predictions)\n",
    "        return {'accuracy': acc, 'f1': f1, 'precision': precision, 'recall': recall}\n",
    "\n",
    "# ensemble them\n",
    "class EnsembleClassifier(BaseEstimator):\n",
    "    def __init__(self, classical_pipeline, codebert_transformer, label_encoder, classical_weight=0.6, codebert_weight=0.4):\n",
    "        self.classical_pipeline = classical_pipeline\n",
    "        self.codebert_transformer = codebert_transformer\n",
    "        self.classical_weight = classical_weight\n",
    "        self.codebert_weight = codebert_weight\n",
    "        self.label_encoder = label_encoder\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Encode labels\n",
    "        y_encoded = y\n",
    "\n",
    "        # Fit classical pipeline\n",
    "        self.classical_pipeline.fit(X, y_encoded)\n",
    "\n",
    "        # Fit CodeBERT transformer\n",
    "        self.codebert_transformer.fit(X, y_encoded)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Get probabilities from classical pipeline\n",
    "        classical_probs = self.classical_pipeline.predict_proba(X)\n",
    "\n",
    "        # Get probabilities from CodeBERT transformer\n",
    "        codebert_probs = self.codebert_transformer.transform(X)\n",
    "\n",
    "        # Ensemble probabilities\n",
    "        total_weight = self.classical_weight + self.codebert_weight\n",
    "        classical_weight = self.classical_weight / total_weight\n",
    "        codebert_weight = self.codebert_weight / total_weight\n",
    "\n",
    "        ensemble_probs = (classical_weight * classical_probs) + (codebert_weight * codebert_probs)\n",
    "\n",
    "        # Get final predictions\n",
    "        y_pred = np.argmax(ensemble_probs, axis=1)\n",
    "        y_pred_labels = self.label_encoder.inverse_transform(y_pred)\n",
    "        return y_pred_labels\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        # Get probabilities from classical pipeline\n",
    "        classical_probs = self.classical_pipeline.predict_proba(X)\n",
    "\n",
    "        # Get probabilities from CodeBERT transformer\n",
    "        codebert_probs = self.codebert_transformer.transform(X)\n",
    "\n",
    "        # Ensemble probabilities\n",
    "        total_weight = self.classical_weight + self.codebert_weight\n",
    "        classical_weight = self.classical_weight / total_weight\n",
    "        codebert_weight = self.codebert_weight / total_weight\n",
    "\n",
    "        ensemble_probs = (classical_weight * classical_probs) + (codebert_weight * codebert_probs)\n",
    "        return ensemble_probs\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Load Data\n",
    "    train_df = pd.read_excel('../Model and Dataset/clean_train.xlsx', engine= 'openpyxl')\n",
    "    test_df = pd.read_excel('../Model and Dataset/clean_test.xlsx', engine= 'openpyxl')\n",
    "\n",
    "    # Combine train and test data for label encoding\n",
    "    combined_df = pd.concat([train_df, test_df], ignore_index=True)\n",
    "\n",
    "    # Encode Labels\n",
    "    label_encoder = LabelEncoder()\n",
    "    combined_df['label'] = label_encoder.fit_transform(combined_df['Expected Operation by Developer'])\n",
    "    train_df['label'] = label_encoder.transform(train_df['Expected Operation by Developer'])\n",
    "    test_df['label'] = label_encoder.transform(test_df['Expected Operation by Developer'])\n",
    "\n",
    "    # Features and Labels\n",
    "    X_train = train_df['Review Comment']\n",
    "    y_train = train_df['label']\n",
    "    X_test = test_df['Review Comment']\n",
    "    y_test = test_df['label']\n",
    "\n",
    "    # Initialize Classical Pipeline Components\n",
    "    vectorizer = CountVectorizer(ngram_range=(1, 2))\n",
    "    selector = SelectKBest(chi2, k=1010)\n",
    "    classifier = LogisticRegression(\n",
    "        multi_class='multinomial',\n",
    "        solver='lbfgs',\n",
    "        max_iter=300000,\n",
    "        C=1,\n",
    "        class_weight='balanced',\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    classical_pipeline = Pipeline([\n",
    "        ('vectorizer', vectorizer),\n",
    "        ('selector', selector),\n",
    "        ('classifier', classifier)\n",
    "    ])\n",
    "\n",
    "    # Initialize CodeBERT Transformer\n",
    "    num_labels = len(label_encoder.classes_)\n",
    "    codebert_transformer = CodeBERTTransformer(num_labels=num_labels)\n",
    "\n",
    "    # Initialize Ensemble Classifier\n",
    "    ensemble_classifier = EnsembleClassifier(\n",
    "        classical_pipeline=classical_pipeline,\n",
    "        codebert_transformer=codebert_transformer,\n",
    "        label_encoder=label_encoder,  # Pass the label encoder\n",
    "        classical_weight=0.6,\n",
    "        codebert_weight=0.4\n",
    "    )\n",
    "\n",
    "    # Fit Ensemble Classifier\n",
    "    ensemble_classifier.fit(X_train, y_train)\n",
    "\n",
    "    # Predict on Test Set\n",
    "    y_pred = ensemble_classifier.predict(X_test)\n",
    "\n",
    "    # Evaluate Ensemble Model\n",
    "    y_true_labels = label_encoder.inverse_transform(y_test)\n",
    "    print(\"\\nClassification Report for Ensemble Model:\")\n",
    "    print(classification_report(y_true_labels, y_pred))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
