{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the initial step, we decided to preprocess both the training data and test data by\n",
    "1. Removing review comments which have high similarity but different class labels in **train data only**.\n",
    "2. Removing review comments that comprise keywords from all three classes, `insert`, `delete`, `replace` in **both train and test data**\n",
    "3. Removing vague review comments in **both train and test data** which do not contains any meaningful context for the model to learn from.\n",
    "\n",
    "We acknowledged that performing preprocessing steps on test data is sometimes referred as **data leakage** or **test set contamination**, which is not the best practice in the industry. However, in the context of code review comment, these filters are essential and necessary in order for the review comments that are possibly ambiguous to be manually reviewed by the Senior Developer. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Import Libraries and Defining Self-Created Dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Dictionary Created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = {\n",
    "    'delete': [\n",
    "        r'\\b(remove|removed|delete|deleted|drop|dropped|eliminate|eliminated)\\b',\n",
    "        r'\\b(unnecessary|redundant)\\b',\n",
    "        r'\\b(get\\s+rid\\s+of|got\\s+rid\\s+of)\\b',\n",
    "        r'\\b(clean\\s+up|cleaned\\s+up)\\b',\n",
    "        r'\\b(nuke|kill|killed)\\b',\n",
    "        r'\\b(wrong|extra|space|trailing|bad|long)\\b'\n",
    "    ],\n",
    "    'replace': [\n",
    "        r'\\b(change|changed|replace|replaced|rename|renamed|convert|converted)\\b',\n",
    "        r'better\\s+to\\s+use',\n",
    "        r'\\b(misplace|misplaced|misuse|misused)\\b',\n",
    "        r'\\b(lowercase|uppercase|spelling|punctuation|punctuations)\\b'\n",
    "    ],\n",
    "    'insert': [\n",
    "        r'\\b(add|added|insert|inserted|include|included)\\b',\n",
    "        r'\\b(missing|need|needed)\\b',\n",
    "        r'needs?\\s+to\\s+be\\s+added',\n",
    "        r'put\\s+in',\n",
    "        r'short',\n",
    "        r'tab',\n",
    "        r'new'\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Filtering Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 2.1 Removing Similar Review Comments with Different Labels \n",
    "\n",
    "\n",
    "There are two stages within this filter function:\n",
    "1. Finding exact duplicates with different labels. \n",
    "2. Finding review comments with weighted word-level similarities and character-level similarities **(7:3)** that exceed the `similarity_threshold` set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similar_comments_w_diff_label(df, similarity_threshold=0.6):\n",
    "    \"\"\"Find similar comments with different class labels.\"\"\"\n",
    "    print(\"Normalizing comments...\")\n",
    "    df['comment_normalized'] = df['Review Comment'].str.lower().str.strip()\n",
    "\n",
    "    # Find exact duplicates with different labels\n",
    "    print(\"Finding exact duplicates...\")\n",
    "    duplicates = df.groupby('comment_normalized')['Expected Operation by Developer'].agg(set)\n",
    "    ambiguous_exact = duplicates[duplicates.apply(len) > 1].index.tolist()\n",
    "\n",
    "    # Find similar comments using character-level similarities\n",
    "    print(\"Calculating character-level similarities...\")\n",
    "    char_vectorizer = TfidfVectorizer(\n",
    "        analyzer='char',\n",
    "        ngram_range=(2, 4),\n",
    "        strip_accents='unicode'\n",
    "    )\n",
    "    char_tfidf = char_vectorizer.fit_transform(df['comment_normalized'])\n",
    "    char_similarities = cosine_similarity(char_tfidf)\n",
    "\n",
    "    # Find similar comments using word-level similarities\n",
    "    print(\"Calculating word-level similarities...\")\n",
    "    word_vectorizer = TfidfVectorizer(\n",
    "        strip_accents='unicode',\n",
    "        ngram_range=(1, 3),\n",
    "        max_features=10000\n",
    "    )\n",
    "    word_tfidf = word_vectorizer.fit_transform(df['comment_normalized'])\n",
    "    word_similarities = cosine_similarity(word_tfidf)\n",
    "\n",
    "    # Combine similarities with more weight on word similarities\n",
    "    similarities = (0.7 * word_similarities + 0.3 * char_similarities)\n",
    "\n",
    "    ambiguous_pairs = []\n",
    "    n_samples = len(df)\n",
    "    for i in range(n_samples):\n",
    "        for j in range(i + 1, n_samples):\n",
    "            if similarities[i, j] > similarity_threshold:\n",
    "                if df.iloc[i]['Expected Operation by Developer'] != df.iloc[j]['Expected Operation by Developer']:\n",
    "                    ambiguous_pairs.append({\n",
    "                        'comment1': df.iloc[i]['Review Comment'],\n",
    "                        'label1': df.iloc[i]['Expected Operation by Developer'],\n",
    "                        'comment2': df.iloc[j]['Review Comment'],\n",
    "                        'label2': df.iloc[j]['Expected Operation by Developer'],\n",
    "                        'similarity_score': similarities[i, j]\n",
    "                    })\n",
    "\n",
    "    # Sort pairs by similarity score\n",
    "    ambiguous_pairs.sort(key=lambda x: x['similarity_score'], reverse=True)\n",
    "\n",
    "    # Get all ambiguous comments\n",
    "    ambiguous_comments = set(ambiguous_exact)  # Start with exact duplicates\n",
    "    for pair in ambiguous_pairs:\n",
    "        ambiguous_comments.add(pair['comment1'].lower().strip())\n",
    "        ambiguous_comments.add(pair['comment2'].lower().strip())\n",
    "\n",
    "    return list(ambiguous_comments), ambiguous_pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 2.2 Removing Review Comments with Keywords from All Three Class Labels\n",
    "\n",
    "\n",
    "We will check through a comment and look for the existence of keywords defined in the dictionary. Within each comment, if there are keywords found each class label, we will considered the `strong_indicators` now has the value of 1. If the `strong_indicators` sum up to be 3, which strongly indicates the possibility of a comment being confusing, and thus we remove it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_all_keywords(comment):\n",
    "    \"\"\"Check if comment contains keywords from dictionary that we defined for each class label.\"\"\"\n",
    "    comment = str(comment).lower()\n",
    "    matches = {}\n",
    "    for op, patterns in dictionary.items():\n",
    "        matches[op] = sum(bool(re.search(pattern, comment)) for pattern in patterns)\n",
    "    strong_indicators = sum(count > 0 for count in matches.values())\n",
    "    return strong_indicators > 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 2.3 Removing Vague or Contextless Review Comments\n",
    "\n",
    "\n",
    "There are two checks within this filter function:\n",
    "1. If the review comment is short `len(words)<4` and contains open-ended keywords.\n",
    "2. If the review comment is very short `len(words)<3` and does not contain keywords for any of the class label defined in the dictionary above.\n",
    "\n",
    "We will remove the comment if it marks any of these two checks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_vague_or_contextless(comment):\n",
    "    \"\"\"Check for vague or contextless comments.\"\"\"\n",
    "    comment = str(comment).lower()\n",
    "    words = comment.split()\n",
    "\n",
    "    questionable_patterns = [\n",
    "        r'^same\\s+(as|like|with)',\n",
    "        r'^(as|like)\\s+above',\n",
    "        r'^(as|like)\\s+before',\n",
    "        r'^(as|like)\\s+previous',\n",
    "        r'similar\\s+to',\n",
    "        r'^why',\n",
    "        r'^what',\n",
    "        r'^when',\n",
    "        r'^where',\n",
    "        r'^how',\n",
    "        r'^these',\n",
    "        r'^this',\n",
    "        r'^those',\n",
    "        r'^that',\n",
    "        r'^shouldn\\'t\\s+',\n",
    "        r'^shouldnt\\s+',\n",
    "        r'^should\\s+not',\n",
    "        r'^couldn\\'t\\s+',\n",
    "        r'^couldnt\\s+',\n",
    "        r'^could\\s+not',\n",
    "        r'^wouldn\\'t\\s+',\n",
    "        r'^wouldnt\\s+',\n",
    "        r'^would\\s+not',\n",
    "    ]\n",
    "\n",
    "    has_questionable = any(re.search(pattern, comment) for pattern in questionable_patterns) and len(words) < 4\n",
    "\n",
    "    keyword = 0\n",
    "    for _, patterns in dictionary.items():\n",
    "        keyword += sum(bool(re.search(pattern, comment)) for pattern in patterns)\n",
    "    contextless = len(words) < 3 and keyword < 1\n",
    "\n",
    "    return has_questionable or contextless\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Correcting Abbreviations and Typos \n",
    "\n",
    "\n",
    "In case the dataset contains abbreviations or typos (due to human errors), we will correct them before the model training so that the data integrity is preserved and do not interfered with the learning process of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load abbreviations\n",
    "def load_abbreviations():\n",
    "    \"\"\"Load and combine code and natural language abbreviations.\"\"\"\n",
    "    # Load code abbreviations\n",
    "    with open(\"../Model and Dataset/abbreviations-in-code/data/abbrs/.json\", 'r') as file:\n",
    "        abbreviations = json.load(file)\n",
    "        dict_for_abbreviations_code = {i['abbrs'][0]['abbr']: i['word'] for i in abbreviations}\n",
    "\n",
    "    # Add natural language abbreviations\n",
    "    dict_for_abbreviations_nl = {\n",
    "        'fwiw': \"for what it's worth\",\n",
    "        'u': \"you\",\n",
    "        \"ur\": \"your\",\n",
    "        \"nit\": \"Nitpicking\"\n",
    "    }\n",
    "\n",
    "    return {**dict_for_abbreviations_code, **dict_for_abbreviations_nl}\n",
    "\n",
    "\n",
    "# Clean text function\n",
    "def clean_text(text, abbreviations_dict):\n",
    "    \"\"\"Replace abbreviations in the text.\"\"\"\n",
    "    # Replace abbreviations\n",
    "    words = text.split()\n",
    "    words = [abbreviations_dict.get(word.lower(), word) for word in words]\n",
    "    text = ' '.join(words)\n",
    "    # Correct typos\n",
    "    text = str(TextBlob(text).correct())\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4: Integrate All Filtering Functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset cleaning function\n",
    "def clean_dataset(df, is_train, save_details=True, verbose=True):\n",
    "    \"\"\"Enhanced dataset cleaning with multiple filtering steps.\"\"\"\n",
    "    original_size = len(df)\n",
    "\n",
    "    # Load abbreviations\n",
    "    print(\"Loading abbreviations...\")\n",
    "    abbreviations_dict = load_abbreviations()\n",
    "\n",
    "    # Initialize masks\n",
    "    similarity_mask = pd.Series(False, index=df.index)\n",
    "    contradictory_mask = pd.Series(False, index=df.index)\n",
    "    ambiguous_mask = pd.Series(False, index=df.index)\n",
    "\n",
    "    if is_train:\n",
    "        print(\"Finding similar comments with different labels...\")\n",
    "        ambiguous_comments, ambiguous_pairs = similar_comments_w_diff_label(df)\n",
    "        similarity_mask = df['Review Comment'].str.lower().str.strip().isin(ambiguous_comments)\n",
    "\n",
    "        # Remove ambiguous comments\n",
    "        df_cleaned = df[~similarity_mask].copy()\n",
    "    else:\n",
    "        df_cleaned = df.copy()\n",
    "\n",
    "    print(\"Applying additional filters...\")\n",
    "    contradictory_mask_cleaned = df_cleaned['Review Comment'].apply(has_all_keywords)\n",
    "    ambiguous_mask_cleaned = df_cleaned['Review Comment'].apply(is_vague_or_contextless)\n",
    "\n",
    "    combined_mask_cleaned = contradictory_mask_cleaned | ambiguous_mask_cleaned\n",
    "    cleaned_df = df_cleaned[~combined_mask_cleaned].copy()\n",
    "\n",
    "    # Apply text cleaning and typo correction to the final cleaned dataset\n",
    "    print(\"Applying text cleaning and typo correction...\")\n",
    "    cleaned_df['Review Comment'] = cleaned_df['Review Comment'].apply(\n",
    "        lambda x: clean_text(x, abbreviations_dict)\n",
    "    )\n",
    "\n",
    "    # Save details if needed\n",
    "    if save_details and is_train:\n",
    "        print(\"Saving details...\")\n",
    "        # Prepare 'removed_df' with comments removed due to similar comments with different labels \n",
    "        removed_similar_df = df[similarity_mask].copy()\n",
    "        removed_similar_df['Removal_Reason'] = 'Similar'\n",
    "\n",
    "        removed_contradictory_ambiguous_df = df_cleaned[combined_mask_cleaned].copy()\n",
    "        removed_contradictory_ambiguous_df['Removal_Reason'] = 'Contradictory/Ambiguous'\n",
    "\n",
    "        removed_df = pd.concat([removed_similar_df, removed_contradictory_ambiguous_df], ignore_index=True)\n",
    "\n",
    "        # Save ambiguous pairs\n",
    "        pd.DataFrame(ambiguous_pairs).to_csv('ambiguous_pairs.csv', index=False)\n",
    "        print(\"Ambiguous pairs saved to 'ambiguous_pairs.csv'.\")\n",
    "\n",
    "        # Save removed examples\n",
    "        removed_df.to_csv('removed_examples.csv', index=False)\n",
    "        print(\"Removed examples saved to 'removed_examples.csv'.\")\n",
    "\n",
    "    # Verbose output\n",
    "    if verbose:\n",
    "        total_removed = original_size - len(cleaned_df)\n",
    "        print(\"\\nCleaning Results:\")\n",
    "        print(f\"Original dataset size: {original_size}\")\n",
    "        print(f\"Cleaned dataset size: {len(cleaned_df)}\")\n",
    "        print(f\"Total removed: {total_removed} ({(total_removed / original_size) * 100:.1f}%)\")\n",
    "\n",
    "        if is_train:\n",
    "            print(f\"- Similar: {similarity_mask.sum()} ({(similarity_mask.sum() / original_size) * 100:.1f}%)\")\n",
    "            print(f\"- Contradictory/Ambiguous: {combined_mask_cleaned.sum()} ({(combined_mask_cleaned.sum() / original_size) * 100:.1f}%)\")\n",
    "        else:\n",
    "            print(f\"- Contradictory/Ambiguous: {combined_mask_cleaned.sum()} ({(combined_mask_cleaned.sum() / original_size) * 100:.1f}%)\")\n",
    "\n",
    "        print(\"\\nClass distribution before cleaning:\")\n",
    "        print(df['Expected Operation by Developer'].value_counts(normalize=True))\n",
    "        print(\"\\nClass distribution after cleaning:\")\n",
    "        print(cleaned_df['Expected Operation by Developer'].value_counts(normalize=True))\n",
    "\n",
    "    return cleaned_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5: Run the main() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "\n",
      "Cleaning training data...\n",
      "Loading abbreviations...\n",
      "Finding similar comments with different labels...\n",
      "Normalizing comments...\n",
      "Finding exact duplicates...\n",
      "Calculating character-level similarities...\n",
      "Calculating word-level similarities...\n",
      "Applying additional filters...\n",
      "Applying text cleaning and typo correction...\n",
      "Saving details...\n",
      "Ambiguous pairs saved to 'ambiguous_pairs.csv'.\n",
      "Removed examples saved to 'removed_examples.csv'.\n",
      "\n",
      "Cleaning Results:\n",
      "Original dataset size: 931\n",
      "Cleaned dataset size: 875\n",
      "Total removed: 56 (6.0%)\n",
      "- Similar: 20 (2.1%)\n",
      "- Contradictory/Ambiguous: 36 (3.9%)\n",
      "\n",
      "Class distribution before cleaning:\n",
      "Expected Operation by Developer\n",
      "insert     0.345865\n",
      "delete     0.331901\n",
      "replace    0.322234\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Class distribution after cleaning:\n",
      "Expected Operation by Developer\n",
      "insert     0.336000\n",
      "delete     0.332571\n",
      "replace    0.331429\n",
      "Name: proportion, dtype: float64\n",
      "Cleaned training data saved to 'clean_train.xlsx'.\n",
      "\n",
      "Cleaning test data...\n",
      "Loading abbreviations...\n",
      "Applying additional filters...\n",
      "Applying text cleaning and typo correction...\n",
      "\n",
      "Cleaning Results:\n",
      "Original dataset size: 234\n",
      "Cleaned dataset size: 221\n",
      "Total removed: 13 (5.6%)\n",
      "- Contradictory/Ambiguous: 13 (5.6%)\n",
      "\n",
      "Class distribution before cleaning:\n",
      "Expected Operation by Developer\n",
      "insert     0.346154\n",
      "delete     0.333333\n",
      "replace    0.320513\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Class distribution after cleaning:\n",
      "Expected Operation by Developer\n",
      "delete     0.334842\n",
      "insert     0.334842\n",
      "replace    0.330317\n",
      "Name: proportion, dtype: float64\n",
      "Cleaned test data saved to 'clean_test.xlsx'.\n",
      "\n",
      "Cleaning process completed!\n"
     ]
    }
   ],
   "source": [
    "# Main function for data cleaning\n",
    "def main():\n",
    "    # Load data\n",
    "    print(\"Loading data...\")\n",
    "    train_df = pd.read_excel('../Model and Dataset/Train.xlsx')\n",
    "    test_df = pd.read_excel('../Model and Dataset/Test.xlsx')\n",
    "\n",
    "    print(\"\\nCleaning training data...\")\n",
    "    clean_train_df = clean_dataset(train_df, is_train=True)\n",
    "    clean_train_df.to_excel('clean_train.xlsx', index=False)\n",
    "    print(\"Cleaned training data saved to 'clean_train.xlsx'.\")\n",
    "\n",
    "    print(\"\\nCleaning test data...\")\n",
    "    clean_test_df = clean_dataset(test_df, is_train=False)\n",
    "    clean_test_df.to_excel('clean_test.xlsx', index=False)\n",
    "    print(\"Cleaned test data saved to 'clean_test.xlsx'.\")\n",
    "\n",
    "    print(\"\\nCleaning process completed!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
